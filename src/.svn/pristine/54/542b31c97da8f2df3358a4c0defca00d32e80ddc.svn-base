package sim.content.request;

import caching.base.AbstractCachingPolicy;
import java.util.Collections;
import java.util.HashMap;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.SortedMap;
import sim.ISynopsisString;
import sim.content.Chunk;
import sim.space.cell.smallcell.SmallCell;
import sim.space.users.CachingUser;
import traces.dmdtrace.TraceWorkloadRecord;
import util.DebugUtils;

/**
 * @author xvas
 */
public class DocumentRequest extends TraceWorkloadRecord implements ISynopsisString, IRequest {

    /**
     * The simulation time of request. This is not the same as the time loaded
     * from the trace workload.
     */
    protected final int _issuedAtSimTime; // either a stationary small cell user or a mobile user
    protected final CachingUser _requesterUser; // either a stationary small cell user or a mobile user
    protected final Map<AbstractCachingPolicy, List<Chunk>> _unconsumedChunksInSequence;
    protected final Map<AbstractCachingPolicy, List<Chunk>> _chunksConsumedHistoryFromMC;
    private final Map<AbstractCachingPolicy, List<Chunk>> _chunksConsumedHistoryFromMCWhenDisconnected;
    protected final Map<AbstractCachingPolicy, List<Chunk>> _chunksConsumedHistoryFromBH;
    protected final Map<AbstractCachingPolicy, List<Chunk>> _chunksHitsHistoryFromSC;

//    List<RequestedChunk> _consumedChunksInSequence = null;
    public DocumentRequest(TraceWorkloadRecord workloadRecord, CachingUser requesterUser) {

        super(workloadRecord.getSim(), workloadRecord.sizeInBytes(), workloadRecord.getID(), workloadRecord.getTime());

        _issuedAtSimTime = simTime();

        _requesterUser = requesterUser;

        _unconsumedChunksInSequence = new HashMap<>();//new TreeMap(referredContentDocument().getChunksInSequence());
        _chunksConsumedHistoryFromMC = new HashMap<>();
        _chunksConsumedHistoryFromMCWhenDisconnected = new HashMap<>();
        _chunksHitsHistoryFromSC = new HashMap<>();
        _chunksConsumedHistoryFromBH = new HashMap<>();

        Collection<Chunk> chunksInSequence = referredContentDocument().getChunksInSequence().values();

        for (AbstractCachingPolicy policy : getSim().getCachingPolicies()) {
            _unconsumedChunksInSequence.put(policy, new ArrayList<>(chunksInSequence));
            _chunksConsumedHistoryFromMC.put(policy, new ArrayList<Chunk>());
            _chunksConsumedHistoryFromMCWhenDisconnected.put(policy, new ArrayList<Chunk>());
            _chunksHitsHistoryFromSC.put(policy, new ArrayList<Chunk>());
            _chunksConsumedHistoryFromBH.put(policy, new ArrayList<Chunk>());
        }
    }

    @Override
    public final String toString() {
        StringBuilder bld = new StringBuilder();
        bld.
                append(super.toString()).
                append("\n\t with hash code value ").
                append(hashCode()).append(", requested by mobile ").
                append(_requesterUser.toSynopsisString()).
                append(" at time ").append(_issuedAtSimTime).
                append(",  referring to Content:\n\t\t").
                append(referredContentDocument().toString());
        return bld.toString();
    }

    @Override
    public final String toSynopsisString() {
        StringBuilder bld = new StringBuilder();
        bld.
                append(super.toString()).
                append("\n\t with hash code value ").
                append(hashCode()).append(", requested by mobile ").
                append(_requesterUser.toSynopsisString()).
                append(" at time ").append(_issuedAtSimTime).
                append(",  referring to Content:\n\t\t").
                append(referredContentDocument().toSynopsisString());
        return bld.toString();
    }

    /**
     * Takes into account the simulation issue time and the hashcode of the
     * superclass. Avoids conflicts with same ID between different records
     * referring to the same item.
     *
     * @return
     */
    @Override
    public int hashCode() {
        int hash = super.hashCode();
        hash = 29 * hash + (int) (Double.doubleToLongBits(getIssuedAtSimTime())
                ^ (Double.doubleToLongBits(getIssuedAtSimTime()) >>> 32));
        return hash;
    }

    /**
     *
     * @param obj
     * @return
     */
    @Override
    public boolean equals(Object obj) {
        if (!super.equals(obj)) {
            return false;
        }
        // super checks for getClass() != obj.getClass(), so it can be casted
        final DocumentRequest other = (DocumentRequest) obj;
        return getIssuedAtSimTime() == other.getIssuedAtSimTime();
    }

    @Override
    public long requesterUserID() {
        return getRequesterUser().getID();
    }

    @Override
    public int getIssuedAtSimTime() {
        return _issuedAtSimTime;
    }

    @Override
    public Map<Chunk, Double> predictChunksToCache(
            double handoverProb,
            double expectedHandoffDuration, double handDurError,
            double expectedResidenceDuration, double resiDurError,
            int mcRateSliceBytes, int bhRateSliceBytes, int scRateSliceBytes) {

        Map<Chunk, Double> chunks = new LinkedHashMap<>();// keep insertion order in returned 

//skip fast
        if (handoverProb == 0) {
            // considering a non-neighbor, which in some scenario setups may be allowed to happen.
            SortedMap<Long, Chunk> chunksInSequence = referredContentDocument().getChunksInSequence();
            long lastSeq = chunksInSequence.lastKey();
            for (long seqNum = 1;
                    seqNum <= lastSeq;
                    seqNum++) {
                
                Chunk chunk = chunksInSequence.get(seqNum);
                chunks.put(chunk, 0.0);
            }
            return chunks;
        }

        double chunkSizeInBytes = getSim().chunkSizeInBytes();

        // Expected values based on parameter values.
        // Needed to compute which chunks wil be requested.
        long consumedFromMCDuringHandoff
                = Math.round(expectedHandoffDuration * mcRateSliceBytes / chunkSizeInBytes);
        long cachableChunksDuringHandoff
                = Math.round(expectedHandoffDuration * bhRateSliceBytes / chunkSizeInBytes);
        long consumableChunksDuringSCConnection
                = Math.round(expectedResidenceDuration * scRateSliceBytes / chunkSizeInBytes);

        long firstChunkSequenceNum
                = Math.round(consumedFromMCDuringHandoff * (1.0 - handDurError) + 1);

        SortedMap<Long, Chunk> chunksInSequence = referredContentDocument().getChunksInSequence();
        long lastSeq = chunksInSequence.lastKey(); // just in case the item is small
        if (lastSeq <= firstChunkSequenceNum) {
            // the content is too small and will be consumed entirelly during the handover

            for (long seqNum = 1;
                    seqNum <= lastSeq;
                    seqNum++) {
                Chunk chunk = chunksInSequence.get(seqNum);
                                

                double tunedProb = handoverProb * seqNum / (firstChunkSequenceNum + 1);
                chunks.put(chunk, tunedProb);

                DebugUtils.printer.print("\nseqNum = " + seqNum);
                DebugUtils.printer.print("\t*handoverProb = " + handoverProb);
                DebugUtils.printer.print("\t*pre tunedProb = " + tunedProb);
            }

            return chunks;
        }

        long cachableChunks = Math.min(
                cachableChunksDuringHandoff,
                consumableChunksDuringSCConnection
        );

        double errorUsed;
        if (cachableChunks == cachableChunksDuringHandoff) {// if due to handover time x BH
            errorUsed = handDurError;
        } else {//if (cachableChunks == consumableChunksDuringSCConnection) { // if due to residence time x SC wireless bandwidth
            errorUsed = resiDurError;
        }

        long lastSeqCached = Math.round(cachableChunks * (1.0 + errorUsed) + firstChunkSequenceNum);
        lastSeqCached = Math.min(lastSeq, lastSeqCached);// just in case the item is small

        for (long seqNum = 1;
                seqNum < firstChunkSequenceNum;
                seqNum++) {
            Chunk chunk = chunksInSequence.get(seqNum);
                            

            double tunedProb = handoverProb * seqNum / (firstChunkSequenceNum + 1);
            chunks.put(chunk, tunedProb);

            DebugUtils.printer.print("\nseqNum = " + seqNum);
            DebugUtils.printer.print("\thandoverProb = " + handoverProb);
            DebugUtils.printer.print("\tpre tunedProb = " + tunedProb);
        }

        for (long seqNum = firstChunkSequenceNum;
                seqNum <= lastSeqCached;
                seqNum++) {
            Chunk chunk = chunksInSequence.get(seqNum);
                            

            chunks.put(chunk, handoverProb);
        }

        for (long seqNum = lastSeqCached + 1;
                seqNum <= lastSeq;
                seqNum++) {
            Chunk chunk = chunksInSequence.get(seqNum);
                            

            double tunedProb = handoverProb * (seqNum - lastSeqCached) / (lastSeq + 1 - lastSeqCached);
            chunks.put(chunk, tunedProb);

            DebugUtils.printer.print("\nseqNum = " + seqNum);
            DebugUtils.printer.print("\thandoverProb = " + handoverProb);
            DebugUtils.printer.print("\tpost tunedProb = " + tunedProb);
        }

        return chunks;
    }

    /**
     * @return the _requesterUser
     */
    @Override
    public CachingUser getRequesterUser() {
        return _requesterUser;
    }

    /**
     *
     * @param mcRateSlice the assumed slice per request of the macro-cellular
     * rate
     * @param fillInWithDownloadedFromMC
     * @param scRateSlice the assumed slice per request of the small-cellular
     * rate
     * @param fillInWithCacheHits
     * @param minSCorBHRateSlice the assumed slice per request of the rate
     * through the backhaul and from the small cell
     * @param fillInWithDownloadedFromBH
     * @param fillInWithMissedPerPolicy
     */
    @Override
    public void consumeChunks(
            double mcRateSlice, Map<AbstractCachingPolicy, List<Chunk>> fillInWithDownloadedFromMC,
            double scRateSlice, Map<AbstractCachingPolicy, List<Chunk>> fillInWithCacheHits,
            double minSCorBHRateSlice, Map<AbstractCachingPolicy, List<Chunk>> fillInWithDownloadedFromBH,
            Map<AbstractCachingPolicy, List<Chunk>> fillInWithMissedPerPolicy) {

        long chunkSizeInBytes = getSim().chunkSizeInBytes();

        boolean userConnected = getRequesterUser().isConnected();

        long maxBudget;

        if (!userConnected) {
            maxBudget = Math.round(mcRateSlice / chunkSizeInBytes);

            merge(fillInWithDownloadedFromMC, consumeFromMCWhenDisconnected(maxBudget));

        } else {// in this case, downloads from all reasources, with this *priority*: 

            // CAUTION! do not change  the priority of the following invokations!    
// First, consume from the cache
            maxBudget = Math.round(scRateSlice / chunkSizeInBytes);
            Map<AbstractCachingPolicy, Set<Chunk>> hitsNowInCachePerPolicy
                    = tryHitsFromCachePerPolicy(maxBudget);
            merge2(fillInWithCacheHits, hitsNowInCachePerPolicy);

// Second, from backhaul  
            maxBudget = Math.round(minSCorBHRateSlice / chunkSizeInBytes);
            Map<AbstractCachingPolicy, Set<Chunk>> consumedNowFromBH
                    = consumeFromBH(
                            maxBudget, hitsNowInCachePerPolicy.size()
                    );
            merge2(fillInWithDownloadedFromBH, consumedNowFromBH);

// Third and last, consume from the macro
            maxBudget = Math.round(mcRateSlice / chunkSizeInBytes);
            merge(fillInWithDownloadedFromMC,
                    consumeChunksFromMC(maxBudget));

        }

    }

    public void cacheForOracle(SmallCell currentlyConnectedSC, double residenceDuration, double howManyChunks) {
        if (!getSim().getCachingPolicies().contains(caching.incremental.Oracle.instance())) {
            return;
        }

        List<Chunk> unconcusmedOracle = _unconsumedChunksInSequence.get(caching.incremental.Oracle.instance());

        if (unconcusmedOracle.isEmpty()) {
            return;
        }

//xxx        List<Chunk> unconcusmed2 = _unconsumedChunksInSequence.get(caching.rplc.mingain.no_price.EMPC_LC_Full.instance());
//        
//         Utils.printer.append("\n>>> first seq in unconcusmedOracle=" + unconcusmedOracle.get(0).getSequenceNum());
//         Utils.printer.append("\n>>> first seq in EMPC_LC_Full unconcusmed2  =" + unconcusmed2.get(0).getSequenceNum());
        Chunk firstSeq = unconcusmedOracle.get(0);

        long firstSeqNum = firstSeq.getSequenceNum();

        long chunksDuringResidence = Math.round(residenceDuration * howManyChunks);

        SortedMap<Long, Chunk> chunksInSequence = referredContentDocument().getChunksInSequence();

        long nxt = firstSeqNum;
        Long lastSeqNum = Math.min(firstSeqNum + chunksDuringResidence, chunksInSequence.lastKey());

//        Utils.printer.append("\n>> chunksDuringResidence=" + chunksDuringResidence);
//xxx        Utils.printer.append("\t residenceDuration=" + residenceDuration);
//        Utils.printer.append("\t firstSeqNum=" + firstSeqNum);
//        Utils.printer.append("\t lastSeqNum=" + lastSeqNum);
        while (nxt < lastSeqNum) {

            Chunk chunkNxt = chunksInSequence.get(nxt++);

            caching.incremental.Oracle.instance().cacheDecision(
                    getRequesterUser(),
                    currentlyConnectedSC,
                    chunkNxt);
        }

    }

    /**
     *
     * @param maxBudget the max number of chunks that can be downloaded from the
     * cache
     * @param fillWithMissedByPolicy for some polices, some chunks may be
     * missed. These chunks must be considered for download by the BH or the MC
     * @return the consumed chunks from the cache
     */
    private Map<AbstractCachingPolicy, Set<Chunk>> tryHitsFromCachePerPolicy(long maxBudget) {
        Map<AbstractCachingPolicy, Set<Chunk>> currentHitsInCachePerPolicy = new HashMap<>();

        for (AbstractCachingPolicy policy : getSim().getCachingPolicies()) {
            long budgetForPolicy = maxBudget;

            Set<Chunk> currentChunkHits;
            currentHitsInCachePerPolicy.put(policy, currentChunkHits = new HashSet<>());
            List<Chunk> historyChunkHits = _chunksHitsHistoryFromSC.get(policy);

            List<Chunk> unconsumed = _unconsumedChunksInSequence.get(policy);
            Iterator<Chunk> unconsumedIt = unconsumed.iterator(); // in ascending order of keys
            Set<Chunk> cachedChunks = _requesterUser.getCurrentlyConnectedSC().cachedChunksUnmodifiable(policy);
            while (unconsumedIt.hasNext() && budgetForPolicy > 0) {
                Chunk chunkConsumed = unconsumedIt.next();
                if (!cachedChunks.contains(chunkConsumed)) {
                    // if not in the cache, skip
                    continue;
                }
                unconsumedIt.remove();
                /* While being connected, only in this case the chunk is not 
                     * already consumed either from the BH nor from MC */
                budgetForPolicy--;
                historyChunkHits.add(chunkConsumed);
                currentChunkHits.add(chunkConsumed);

//                if (getSim().__tmpMaxPopSetXXX2.contains(nxtChunk)) {
//                    throw new RuntimeException();
//                }
//                if (getSim().__tmpMaxPopSetXXX.contains(nxtChunk.referredContentDocument())) {
//                    throw new RuntimeException();
//                }
            }
//            if (_requesterUser.getClass() == StationaryUser.class
//                    && policy.getClass() == EMPC_LC_Full.class) {
//xxx
//                Utils.trackUser(false,
//                        "\n\t\t budget remained: " + policyBudget
//                        + " out of a maxBudget: " + maxBudget
//                        + "\n\t\t hits now : " + nowChunkHits.size()
//                        + "\n\t\t hits history : " + hitsByPolicyHistory.size(),
//                        getRequesterUser(), true);
//            }
        }

        return currentHitsInCachePerPolicy;
    }

    private Map<AbstractCachingPolicy, Set<Chunk>> consumeFromBH(
            long maxbudget, int hitsNum) {

        Map<AbstractCachingPolicy, Set<Chunk>> bhCurrentConsumptionPerPolicy = new HashMap<>();

        for (AbstractCachingPolicy policy : getSim().getCachingPolicies()) {

            Set<Chunk> bhCurrentConsumption;
            bhCurrentConsumptionPerPolicy.put(policy, bhCurrentConsumption = new HashSet<>());
            List<Chunk> bhHistororyConsumption = _chunksConsumedHistoryFromBH.get(policy);

            // each policy has its own budget, i.e. if you have 10 slots in the
            // sc wireless and you have 4 hits in the cache, then you have
            // consumed 4 slots in the wireless. Thus, now you can use only 
            // six slots for this policy.
            long budgetForPolicy = maxbudget - hitsNum;

//xxx            if (_requesterUser.getClass() == StationaryUser.class
//                    && policy.getClass() == EMPC_LC_Full.class) {
//
//                        Utils.trackUser(false,
//                        "\n\t BH: budgetForPolicy = policyBudget /*maxbudget*/  - hitsNum = "
//                        + policyBudget + "/*" + maxbudget + "*/ -" + hitsNum + "="
//                        + budgetForPolicy,
//                        getRequesterUser(), true);
//            }            // if room in HB rate slice, then consume from the unconsumed chunks
//            long xxx = budgetForPolicy;
            Iterator<Chunk> unconsumedIt = _unconsumedChunksInSequence.get(policy).iterator(); // in ascending order of keys
            Set<Chunk> cachedChunks = _requesterUser.getCurrentlyConnectedSC().cachedChunksUnmodifiable(policy);
            while (unconsumedIt.hasNext() && budgetForPolicy > 0) {
                Chunk chunkConsumed = unconsumedIt.next();
                if (cachedChunks.contains(chunkConsumed)) {
                    // if already in the cache, let be consumed in the future from there
                    continue;
                }
//                xxx++;

                unconsumedIt.remove();
                budgetForPolicy--;
                bhHistororyConsumption.add(chunkConsumed);
                bhCurrentConsumption.add(chunkConsumed);
            }
//xxx            if (_requesterUser.getClass() == StationaryUser.class
//                    && policy.getClass() == EMPC_LC_Full.class) {
//                Utils.trackUser(false,
//                        "\n\t\t consumed from BH: " + (xxx - budgetForPolicy),
//                        getRequesterUser(), true);
//            }
        }

        return bhCurrentConsumptionPerPolicy;
    }

    protected Map<AbstractCachingPolicy, List<Chunk>> consumeChunksFromMC(
            long policyBudget) {

        Map<AbstractCachingPolicy, List<Chunk>> mcCurrentConsumptionPerPolicy = new HashMap<>();

        for (AbstractCachingPolicy policy : getSim().getCachingPolicies()) {
            long budgetForPolicy = policyBudget;

            List<Chunk> mcCurrentConsumption;
            mcCurrentConsumptionPerPolicy.put(policy, mcCurrentConsumption = new ArrayList<>());
            List<Chunk> mcHistororyConsumption = _chunksConsumedHistoryFromMC.get(policy);

//xxx            int xxx = 0;
//            if (_requesterUser.getClass() == StationaryUser.class
//                    && policy.getClass() == EMPC_LC_Full.class) {
//                Utils.trackUser(false,
//                        "\t #MC policyBudget " + policyBudget,
//                        getRequesterUser(), true);
//            }
            Iterator<Chunk> unconsumedIt = _unconsumedChunksInSequence.get(policy).iterator(); // in ascending order of keys
            Set<Chunk> cachedChunks = _requesterUser.getCurrentlyConnectedSC().cachedChunksUnmodifiable(policy);
            while (unconsumedIt.hasNext() && budgetForPolicy > 0) {
                Chunk chunkConsumed = unconsumedIt.next();
                if (cachedChunks.contains(chunkConsumed)) {
                    // if already in the cache, let be consumed in the future from there
                    continue;
                }
//                xxx++;

                unconsumedIt.remove();
                budgetForPolicy--;
                mcHistororyConsumption.add(chunkConsumed);
                mcCurrentConsumption.add(chunkConsumed);
            }
//xxx
//            if (_requesterUser.getClass() == StationaryUser.class
//                    && policy.getClass() == EMPC_LC_Full.class) {
//                Utils.trackUser(false,
//                        "\t #MC from rest " + xxx,
//                        getRequesterUser(), true);
//            }
        }
        return mcCurrentConsumptionPerPolicy;
    }

    protected Map<AbstractCachingPolicy, List<Chunk>> consumeFromMCWhenDisconnected(long budget) {

        Map<AbstractCachingPolicy, List<Chunk>> toReturn = new HashMap<>();

//    xxx    Utils.printer.print("\n POLICIES: " + getSim().getCachingPolicies().toString());
        for (AbstractCachingPolicy policy : getSim().getCachingPolicies()) {
            long policyBudget = budget;

            List<Chunk> nowDownloadedChunks;
            toReturn.put(policy, nowDownloadedChunks = new ArrayList<>());
            List<Chunk> consumedHistoryByPolicy = _chunksConsumedHistoryFromMCWhenDisconnected.get(policy);

            Iterator<Chunk> unconsumedIt = _unconsumedChunksInSequence.get(policy).iterator(); // in ascending order of keys

            while (unconsumedIt.hasNext() && policyBudget-- > 0) {
                Chunk chunkConsumed = unconsumedIt.next();
                unconsumedIt.remove();

// xxx               Utils.printer.print("\n"
//                        + "$$" + policy.getClass()
//                        + " consumed " + chunkConsumed.getSequenceNum()
//                );
                consumedHistoryByPolicy.add(chunkConsumed);
                nowDownloadedChunks.add(chunkConsumed);
            }

        }
        return toReturn;
    }

    /**
     * @return the _chunksConsumedHistoryFromMC
     */
    @Override
    public Map<AbstractCachingPolicy, List<Chunk>> getChunksConsumedHistoryFromMC() {
        return Collections.unmodifiableMap(_chunksConsumedHistoryFromMC);
    }

    public Map<AbstractCachingPolicy, List<Chunk>> getChunksConsumedHistoryFromMCWhenDisconnected() {
        return Collections.unmodifiableMap(_chunksConsumedHistoryFromMCWhenDisconnected);
    }

    @Override
    public List<Chunk> getChunksConsumedHistoryFromMC(AbstractCachingPolicy policy) {
        return Collections.unmodifiableList(_chunksConsumedHistoryFromMC.get(policy));
    }

    @Override
    public List<Chunk> getChunksConsumedHistoryFromMCWhenDisconnected(AbstractCachingPolicy policy) {
        return Collections.unmodifiableList(_chunksConsumedHistoryFromMCWhenDisconnected.get(policy));
    }

    /**
     * @param policy
     * @return the _consumedChunksFromSC
     */
    @Override
    public List<Chunk> getChunksCacheHitsHistory(AbstractCachingPolicy policy) {
        return Collections.unmodifiableList(_chunksHitsHistoryFromSC.get(policy));
    }

    /**
     * @return the _chunksConsumedHistoryFromBH
     */
    @Override
    public Map<AbstractCachingPolicy, List<Chunk>> getChunksConsumedHistoryFromBH() {
        return Collections.unmodifiableMap(_chunksConsumedHistoryFromBH);
    }

    @Override
    public List<Chunk> getChunksConsumedHistoryFromBH(AbstractCachingPolicy policy) {
        return Collections.unmodifiableList(_chunksConsumedHistoryFromBH.get(policy));
    }

    /**
     * Merges the value entries in the second to the value entries in the first
     *
     * @param firstMap
     * @param secondMap
     */
    protected final void merge(Map<AbstractCachingPolicy, List<Chunk>> firstMap,
            Map<AbstractCachingPolicy, List<Chunk>> secondMap) {
        for (Map.Entry<AbstractCachingPolicy, List<Chunk>> entry : secondMap.entrySet()) {
            AbstractCachingPolicy policy = entry.getKey();
            Collection<Chunk> chunksInSecond = entry.getValue();

            List<Chunk> chunksInFirst = firstMap.get(policy);
            if (chunksInFirst == null) {
                firstMap.put(policy, (chunksInFirst = new ArrayList<>()));
            }
            chunksInFirst.addAll(chunksInSecond);
        }
    }

    protected final void merge2(Map<AbstractCachingPolicy, List<Chunk>> firstMap,
            Map<AbstractCachingPolicy, Set<Chunk>> secondMap) {
        for (Map.Entry<AbstractCachingPolicy, Set<Chunk>> entry : secondMap.entrySet()) {
            AbstractCachingPolicy policy = entry.getKey();
            Collection<Chunk> chunksInSecond = entry.getValue();

            List<Chunk> chunksInFirst = firstMap.get(policy);
            if (chunksInFirst == null) {
                firstMap.put(policy, (chunksInFirst = new ArrayList<>()));
            }
            chunksInFirst.addAll(chunksInSecond);
        }
    }

}
